# DPPO-EBM å®Œå…¨æ›¿ä»£ç¯å¢ƒå¥–åŠ±ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•ä½¿ç”¨ç®€å•çš„ MLP EBM æ¨¡å‹å®Œå…¨æ›¿ä»£ç¯å¢ƒå¥–åŠ±è¿›è¡Œ DPPO è®­ç»ƒã€‚

## ç³»ç»Ÿæ¦‚è§ˆ

EBM å¥–åŠ±æ›¿ä»£ç³»ç»ŸåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š

1. **Simple MLP EBM**: åŸºäºå¤šå±‚æ„ŸçŸ¥æœºçš„èƒ½é‡æ¨¡å‹
2. **Energy Scaling**: æ¯ä¸ªå»å™ªæ­¥éª¤çš„èƒ½é‡å½’ä¸€åŒ–
3. **Reward Replacement**: å®Œå…¨ç”¨ EBM è®¡ç®—çš„æ•ˆç”¨æ›¿ä»£ç¯å¢ƒå¥–åŠ±
4. **Training Pipeline**: ç«¯åˆ°ç«¯çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹

## ç³»ç»Ÿæ¶æ„

### EBM å¥–åŠ±æ›¿ä»£åŸç†

```
ç¯å¢ƒå¥–åŠ± R_env(s,a) â†’ EBM æ•ˆç”¨ U_EBM(s,a,k)

å…¶ä¸­:
U_EBM(s,a,k) = -(E_Î¸(s,a,k) - Î²_BC(s,k)) / Ï„_k

- E_Î¸(s,a,k): EBM é¢„æµ‹çš„èƒ½é‡å€¼  
- Î²_BC(s,k): åŸºçº¿ç­–ç•¥çš„èƒ½é‡æœŸæœ›
- Ï„_k: ç¬¬ k æ­¥çš„èƒ½é‡ç¼©æ”¾å› å­
```

### æ”¯æŒçš„å¥–åŠ±æ¨¡å¼

- **k0 æ¨¡å¼**: ä»…ä½¿ç”¨æœ€ç»ˆå»å™ªæ­¥éª¤ (k=0) çš„æ•ˆç”¨
- **dense æ¨¡å¼**: å¯¹æ‰€æœ‰å»å™ªæ­¥éª¤çš„æ•ˆç”¨è¿›è¡ŒåŠ æƒæ±‚å’Œ

## æ–‡ä»¶ç»“æ„

```
DPPO-EBM/
â”œâ”€â”€ model/
â”‚   â””â”€â”€ ebm/
â”‚       â””â”€â”€ simple_mlp_ebm.py          # ç®€å• MLP EBM å®ç°
â”œâ”€â”€ cfg/gym/finetune/
â”‚   â”œâ”€â”€ hopper-v2/
â”‚   â”‚   â””â”€â”€ ft_ppo_diffusion_mlp_ebm_reward_only.yaml
â”‚   â”œâ”€â”€ walker2d-v2/
â”‚   â”‚   â””â”€â”€ ft_ppo_diffusion_mlp_ebm_reward_only.yaml
â”‚   â””â”€â”€ halfcheetah-v2/
â”‚       â””â”€â”€ ft_ppo_diffusion_mlp_ebm_reward_only.yaml
â”œâ”€â”€ train_simple_mlp_ebm.py            # EBM è®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_ebm_reward_replacement.py     # æµ‹è¯•è„šæœ¬
â””â”€â”€ checkpoints/                       # EBM æ¨¡å‹æ£€æŸ¥ç‚¹
```

## ä½¿ç”¨æ­¥éª¤

### ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒ EBM æ¨¡å‹

```bash
# ä¸ºæ¯ä¸ªç¯å¢ƒè®­ç»ƒ EBM
python train_simple_mlp_ebm.py --env_name hopper --epochs 50
python train_simple_mlp_ebm.py --env_name walker2d --epochs 50  
python train_simple_mlp_ebm.py --env_name halfcheetah --epochs 50

# ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒ
python train_simple_mlp_ebm.py \
    --env_name hopper \
    --data_path /path/to/trajectory_data.npz \
    --epochs 100 \
    --batch_size 512 \
    --lr 1e-3
```

**è®­ç»ƒæ•°æ®æ ¼å¼**:
- `states`: [N, obs_dim] çŠ¶æ€è§‚æµ‹
- `actions`: [N, horizon_steps, action_dim] åŠ¨ä½œåºåˆ—  
- `rewards`: [N] å¥–åŠ±å€¼

### ç¬¬äºŒæ­¥ï¼šéªŒè¯ EBM æ¨¡å‹

```bash
# è¿è¡Œæµ‹è¯•éªŒè¯æ‰€æœ‰ç»„ä»¶
python test_ebm_reward_replacement.py
```

é¢„æœŸè¾“å‡ºï¼š
```
ğŸ‰ All tests passed! EBM reward replacement is ready to use.
```

### ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œ DPPO è®­ç»ƒ

```bash
# ä½¿ç”¨ EBM å®Œå…¨æ›¿ä»£ç¯å¢ƒå¥–åŠ±
python script/run.py --config-name=ft_ppo_diffusion_mlp_ebm_reward_only \
    --config-dir=cfg/gym/finetune/hopper-v2

python script/run.py --config-name=ft_ppo_diffusion_mlp_ebm_reward_only \
    --config-dir=cfg/gym/finetune/walker2d-v2

python script/run.py --config-name=ft_ppo_diffusion_mlp_ebm_reward_only \
    --config-dir=cfg/gym/finetune/halfcheetah-v2
```

## é…ç½®è¯¦è§£

### EBM æ¨¡å‹é…ç½®

```yaml
model:
  ebm:
    _target_: model.ebm.simple_mlp_ebm.EBMWrapper
    obs_dim: 11                    # è§‚æµ‹ç»´åº¦
    action_dim: 3                  # åŠ¨ä½œç»´åº¦  
    horizon_steps: 4               # åŠ¨ä½œåºåˆ—é•¿åº¦
    hidden_dims: [512, 256, 128]   # éšè—å±‚ç»´åº¦
    activation: "gelu"             # æ¿€æ´»å‡½æ•°
    use_layer_norm: true           # å±‚å½’ä¸€åŒ–
    dropout: 0.1                   # Dropout ç‡
    use_time_embedding: true       # æ—¶é—´æ­¥åµŒå…¥
    max_timesteps: 1000           # æœ€å¤§æ—¶é—´æ­¥
    max_denoising_steps: 20       # æœ€å¤§å»å™ªæ­¥éª¤
```

### å¥–åŠ±æ›¿ä»£é…ç½®

```yaml
model:
  # ç¦ç”¨ PBRSï¼Œåªä½¿ç”¨å¥–åŠ±æ›¿ä»£
  use_ebm_reward_shaping: false
  
  # å¯ç”¨èƒ½é‡ç¼©æ”¾
  use_energy_scaling: true
  energy_scaling_momentum: 0.99
  energy_scaling_use_mad: true
  
  # EBM å¥–åŠ±æ›¿ä»£è®¾ç½®
  use_ebm_reward: true
  ebm_reward_mode: k0              # k0 æˆ– dense
  ebm_reward_clip_u_max: 5.0       # æ•ˆç”¨è£å‰ªé˜ˆå€¼
  ebm_reward_lambda: 2.0           # æ•ˆç”¨ç¼©æ”¾å› å­
  ebm_reward_baseline_M: 8         # åŸºçº¿æ ·æœ¬æ•°
  ebm_reward_baseline_use_mu_only: false  # åŸºçº¿é‡‡æ ·ç­–ç•¥
```

## EBM æ¨¡å‹ç‰¹æ€§

### Simple MLP EBM æ¶æ„

- **è¾“å…¥**: çŠ¶æ€ + åŠ¨ä½œåºåˆ— + æ—¶é—´åµŒå…¥ + å»å™ªæ­¥éª¤åµŒå…¥
- **ç½‘ç»œ**: å¤šå±‚æ„ŸçŸ¥æœº + å±‚å½’ä¸€åŒ– + Dropout
- **è¾“å‡º**: æ ‡é‡èƒ½é‡å€¼
- **æ¸©åº¦å‚æ•°**: å¯å­¦ä¹ çš„èƒ½é‡ç¼©æ”¾å› å­

### è®­ç»ƒç‰¹ç‚¹

- **æŸå¤±å‡½æ•°**: MSE å›å½’æˆ–æ’åºæŸå¤±
- **ä¼˜åŒ–å™¨**: Adam + å­¦ä¹ ç‡è¡°å‡
- **æ­£åˆ™åŒ–**: æƒé‡è¡°å‡ + Dropout + æ¢¯åº¦è£å‰ª
- **æ•°æ®**: æ”¯æŒçœŸå®è½¨è¿¹æ•°æ®å’Œåˆæˆæ•°æ®

## ç›‘æ§ä¸è°ƒè¯•

### è®­ç»ƒæŒ‡æ ‡

- **Loss**: EBM é¢„æµ‹æŸå¤±
- **Correlation**: èƒ½é‡ä¸å¥–åŠ±çš„ç›¸å…³æ€§
- **Temperature**: èƒ½é‡ç¼©æ”¾æ¸©åº¦
- **Energy Range**: èƒ½é‡å€¼çš„èŒƒå›´

### WandB æ—¥å¿—

```yaml
wandb:
  project: gym-${env_name}-finetune-ebm
  run: ${name}_EBM_Reward_Only
  tags: [DPPO+EBM-Reward-Only]
```

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè®°å½•ï¼š
- è®­ç»ƒå’ŒéªŒè¯æŸå¤±
- èƒ½é‡ä¸å¥–åŠ±çš„ç›¸å…³æ€§
- æ¸©åº¦å‚æ•°å˜åŒ–
- åŸå§‹ç¯å¢ƒå¥–åŠ± vs EBM å¥–åŠ±

### è°ƒè¯•æŠ€å·§

1. **æ£€æŸ¥èƒ½é‡ç›¸å…³æ€§**: EBM èƒ½é‡åº”ä¸è´Ÿå¥–åŠ±å‘ˆæ­£ç›¸å…³
2. **ç›‘æ§æ¸©åº¦å‚æ•°**: æ¸©åº¦è¿‡é«˜æˆ–è¿‡ä½å¯èƒ½å¯¼è‡´ä¸ç¨³å®š
3. **éªŒè¯åŸºçº¿è®¡ç®—**: ç¡®ä¿åŸºçº¿ç­–ç•¥èƒ½é‡è®¡ç®—æ­£ç¡®
4. **è§‚å¯Ÿå¥–åŠ±åˆ†å¸ƒ**: EBM å¥–åŠ±åº”åœ¨åˆç†èŒƒå›´å†…

## æ€§èƒ½è°ƒä¼˜

### EBM æ¨¡å‹è°ƒä¼˜

- **ç½‘ç»œå®¹é‡**: å¢åŠ éšè—å±‚ç»´åº¦æé«˜è¡¨è¾¾èƒ½åŠ›
- **æ¿€æ´»å‡½æ•°**: GELU é€šå¸¸æ¯” ReLU è¡¨ç°æ›´å¥½
- **Dropout**: 0.1-0.2 çš„ dropout æœ‰åŠ©äºæ³›åŒ–
- **æ‰¹æ¬¡å¤§å°**: è¾ƒå¤§æ‰¹æ¬¡æœ‰åŠ©äºç¨³å®šè®­ç»ƒ

### å¥–åŠ±æ›¿ä»£è°ƒä¼˜

- **æ•ˆç”¨è£å‰ª**: é˜²æ­¢æç«¯æ•ˆç”¨å€¼å½±å“è®­ç»ƒç¨³å®šæ€§
- **ç¼©æ”¾å› å­**: è°ƒèŠ‚ EBM å¥–åŠ±çš„æ•´ä½“å¹…åº¦
- **åŸºçº¿æ ·æœ¬æ•°**: æ›´å¤šæ ·æœ¬æé«˜åŸºçº¿ä¼°è®¡ç²¾åº¦
- **èƒ½é‡ç¼©æ”¾**: MAD é€šå¸¸æ¯”æ ‡å‡†å·®æ›´ç¨³å¥

## å®éªŒå¯¹æ¯”

### åŸºçº¿å¯¹æ¯”

å¯ä»¥ä¸ä»¥ä¸‹é…ç½®è¿›è¡Œå¯¹æ¯”ï¼š

1. **æ ‡å‡† DPPO**: `ft_ppo_diffusion_mlp_no_ebm.yaml`
2. **PBRS å¢å¼º**: `ft_ppo_diffusion_ebm_mlp.yaml` (å¯ç”¨ PBRS)
3. **EBM å¥–åŠ±æ›¿ä»£**: `ft_ppo_diffusion_mlp_ebm_reward_only.yaml`

### è¯„ä¼°æŒ‡æ ‡

- **Episode Reward**: çœŸå®ç¯å¢ƒå¥–åŠ± (ç”¨äºå…¬å¹³æ¯”è¾ƒ)
- **EBM Reward**: EBM è®¡ç®—çš„æ•ˆç”¨å€¼
- **Success Rate**: ä»»åŠ¡æˆåŠŸç‡ (å¦‚æœé€‚ç”¨)
- **Training Stability**: è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **EBM è®­ç»ƒä¸æ”¶æ•›**
   - æ£€æŸ¥æ•°æ®è´¨é‡å’Œæ ‡ç­¾æ­£ç¡®æ€§
   - é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ æ­£åˆ™åŒ–
   - ä½¿ç”¨æ›´ç®€å•çš„ç½‘ç»œæ¶æ„

2. **å¥–åŠ±æ›¿ä»£ä¸ç¨³å®š**
   - è°ƒæ•´æ•ˆç”¨è£å‰ªé˜ˆå€¼
   - å¢åŠ åŸºçº¿æ ·æœ¬æ•°
   - ä½¿ç”¨ MAD è€Œéæ ‡å‡†å·®è¿›è¡Œç¼©æ”¾

3. **è®­ç»ƒæ€§èƒ½ä¸‹é™**
   - æ£€æŸ¥ EBM ä¸ç¯å¢ƒå¥–åŠ±çš„ç›¸å…³æ€§
   - éªŒè¯å½’ä¸€åŒ–ç»Ÿè®¡æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„æ­£ç¡®

### è°ƒè¯•å‘½ä»¤

```bash
# æ£€æŸ¥ EBM æ¨¡å‹
python -c "
import torch
from model.ebm.simple_mlp_ebm import EBMWrapper
model = EBMWrapper(obs_dim=11, action_dim=3, horizon_steps=4)
print(f'Model parameters: {sum(p.numel() for p in model.parameters())}')"

# éªŒè¯é…ç½®
python test_ebm_reward_replacement.py

# æ£€æŸ¥æ£€æŸ¥ç‚¹
python -c "
import torch
ckpt = torch.load('checkpoints/simple_mlp_ebm_hopper.pt')
print(f'Val loss: {ckpt[\"val_loss\"]:.4f}')
print(f'Epoch: {ckpt[\"epoch\"]}')"
```

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ EBM æ¶æ„

å¯ä»¥é€šè¿‡ä¿®æ”¹ `SimpleMLPEBM` ç±»æ¥å®ç°è‡ªå®šä¹‰æ¶æ„ï¼š

```python
class CustomEBM(SimpleMLPEBM):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # æ·»åŠ è‡ªå®šä¹‰å±‚
        self.custom_layer = nn.Linear(...)
    
    def forward(self, k_idx, t_idx, views, poses, actions):
        # è‡ªå®šä¹‰å‰å‘ä¼ æ’­
        pass
```

### å¤šç¯å¢ƒ EBM

å¯ä»¥è®­ç»ƒä¸€ä¸ª EBM å¤„ç†å¤šä¸ªç¯å¢ƒï¼š

```python
# è”åˆè®­ç»ƒå¤šç¯å¢ƒæ•°æ®
python train_simple_mlp_ebm.py \
    --env_name multi \
    --data_path combined_data.npz \
    --obs_dim 17 \
    --action_dim 6
```

### åœ¨çº¿ EBM æ›´æ–°

å¯ä»¥åœ¨ DPPO è®­ç»ƒè¿‡ç¨‹ä¸­æŒç»­æ›´æ–° EBMï¼š

```yaml
model:
  ebm_online_update: true
  ebm_update_freq: 100
  ebm_update_lr: 1e-4
```

è¿™ä¸ªç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ EBM å¥–åŠ±æ›¿ä»£è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå®Œå…¨æ›¿ä»£ç¯å¢ƒå¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚